{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21caf9",
   "metadata": {},
   "source": [
    "#### 模型定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM 模型需要的输入包括 噪声图像xt 和 时间步t ，输出为预测的噪声ϵθ(xt,t)。\n",
    "\n",
    "首先，我们定义一个时间嵌入层，它负责将时间信息注入到特征中，将 时间步t 映射为高维向量。\n",
    "参考 Transformer 中的位置编码方法，使用正余弦函数将时间步映射到高维空间。公式为：\n",
    "                        PE(t, 2i) = sin(t / 10000^(2i/d))\n",
    "                        PE(t, 2i+1) = cos(t / 10000^(2i/d))\n",
    "其中，d为嵌入维度，t为维度索引。                    \n",
    "\"\"\"\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "\n",
    "        # 将维度分为两半，分别用于sin和cos\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        # 计算不同频率的指数衰减\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "\n",
    "        # 生成频率序列\n",
    "        embeddings = torch.exp(\n",
    "            torch.arange(half_dim, device=device) * -embeddings\n",
    "        )\n",
    "\n",
    "        # 将时间步与频率序列相乘\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "\n",
    "        # 拼接sin和cos得到最终的嵌入向量\n",
    "        embeddings = torch.cat(\n",
    "            (embeddings.sin(), embeddings.cos()),\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf80a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "接着，定义一个U-Net的基本模块Block，包含时间嵌入、上、下采样功能。\n",
    "\n",
    "第一次卷积扩展通道数，然后加入时间嵌入，接着进行第二次卷积，融合特征信息，最后进行上、下采样。\n",
    "\n",
    "注：这里使用简化版U-Net，未使用原文中带有注意力机制的模型。\n",
    "\"\"\"\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_channels, out_channels, kernel_size=3, padding=1)     # 由于 U-Net 的残差连接,上采样时会 concat 之前的特征，输入通道数需要翻倍\n",
    "            self.transform = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # 第一次卷积\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "\n",
    "        # 时间嵌入\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "\n",
    "        # 将时间信息注入特征图\n",
    "        h = h + time_emb[..., None, None]\n",
    "\n",
    "        # 第二次卷积\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "\n",
    "        # 上采样或下采样\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee40c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "最后，将多个Block组合起来，形成一个U-Net模型。\n",
    "每一层都会加入时间步信息，最终输出与输入图像尺寸相同的预测噪声。\n",
    "\"\"\"\n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # 时间嵌入层\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        # 输入层、下采样层、上采样层和输出层\n",
    "        self.input = nn.Conv2d(image_channels, down_channels[0], kernel_size=3, padding=1)\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i + 1], time_emb_dim) for i in range(len(down_channels) - 1)])\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True) for i in range(len(up_channels) - 1)])\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time_step):\n",
    "        # 时间步嵌入\n",
    "        t = self.time_embed(time_step)\n",
    "\n",
    "        # 初步卷积\n",
    "        x = self.input(x)\n",
    "\n",
    "        # UNet前向传播：先下采样收集特征，再上采样恢复分辨率\n",
    "        residual_stack = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_stack.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_stack.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        \n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961370fc",
   "metadata": {},
   "source": [
    "#### 训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "首先需要定义一个噪声调度器，用于控制加噪过程，生成不同时间步的噪声图像。\n",
    "\"\"\"\n",
    "class Noisescheduler(nn.Module):\n",
    "    \"\"\"\n",
    "    在前向过程中，需要定义变量。\n",
    "    这里使用 register_buffer 来定义变量，这样这些变量就会自动与模型参数一起保存和加载。\n",
    "    \"\"\"\n",
    "    def __init__(self, beta_start=0.0001, beta_end=0.02, num_steps=1000):\n",
    "        super().__init__()\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # β_t: 线性噪声调度\n",
    "        self.register_buffer('betas', torch.linspace(beta_start, beta_end, num_steps))\n",
    "        # α_t = 1 - β_t \n",
    "        self.register_buffer('alphas', 1.0 - self.betas)\n",
    "        # α_bar_t = ∏(1-β_i) from i=1 to t\n",
    "        self.register_buffer('alpha_bar', torch.cumprod(self.alphas, dim=0))\n",
    "        # α_bar_(t-1)\n",
    "        self.register_buffer('alpha_bar_prev', torch.cat([torch.tensor([1.0]), self.alpha_bar[:-1]], dim=0))\n",
    "        # sqrt(α_bar_t)\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(self.alpha_bar))\n",
    "        # 1/sqrt(α_t)\n",
    "        self.register_buffer('sqrt_recip_alphas', torch.sqrt(1.0 / self.alphas))\n",
    "        # sqrt(1-α_bar_t)\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1.0 - self.alpha_bar))\n",
    "\n",
    "        # 1/sqrt(α_bar_t)\n",
    "        self.register_buffer('sqrt_recip_alphas_bar', torch.sqrt(1.0 / self.alpha_bar))\n",
    "        # sqrt(1/α_bar_t - 1)\n",
    "        self.register_buffer('sqrt_recipm1_alphas_bar', torch.sqrt(1.0 / self.alpha_bar - 1))\n",
    "        # 后验分布方差 σ_t^2\n",
    "        self.register_buffer('posterior_var', self.betas * (1.0 - self.alpha_bar_prev) / (1.0 - self.alpha_bar))\n",
    "        # 后验分布均值系数1: β_t * sqrt(α_bar_(t-1))/(1-α_bar_t)\n",
    "        self.register_buffer('posterior_mean_coef1', self.betas * torch.sqrt(self.alpha_bar_prev) / (1.0 - self.alpha_bar))\n",
    "        # 后验分布均值系数2: (1-α_bar_(t-1)) * sqrt(α_t)/(1-α_bar_t)\n",
    "        self.register_buffer('posterior_mean_coef2', (1.0 - self.alpha_bar_prev) * torch.sqrt(self.alphas) / (1.0 - self.alpha_bar))\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    由于是对一个batch的图像进行训练，而且需要将这些变量与图像张量进行运算，\n",
    "    而目前定义的张量都是一维张量，所以需要对公式中的变量的维度进行调整，以适应不同张量的维度。\n",
    "\n",
    "    因此定义get方法，用于获取指定时间步的变量值并调整形状，\n",
    "    其中 var 为变量张量，t 为时间步，x_shape 为目标形状。\n",
    "    \"\"\"\n",
    "    def get(self, var, t, x_shape):\n",
    "        # 从变量张量中收集指定时间步的值\n",
    "        out = var[t]\n",
    "\n",
    "        # 调整形状为[batch_size, 1, 1, 1]，以便进行广播\n",
    "        return out.view(\n",
    "            [t.shape[0]] + [1] * (len(x_shape) - 1)\n",
    "        )\n",
    "\n",
    "    # 然后就可以实现加噪过程\n",
    "    def add_noise(self, x, t):\n",
    "        # 获取时间步t对应的sqrt(α_bar_t)\n",
    "        sqrt_alpha_bar = self.get(self.sqrt_alpha_bar, t, x.shape)\n",
    "\n",
    "        # 获取时间步t对应的sqrt(1-α_bar_t)\n",
    "        sqrt_one_minus_alpha_bar = self.get(self.sqrt_one_minus_alpha_bar, t, x.shape)\n",
    "\n",
    "        # 从标准正态分布采样噪声 ε ~ N(0,I)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        # 实现前向扩散过程：x_t = sqrt(α_bar_t) * x_0 + sqrt(1-α_bar_t) * ε\n",
    "        return sqrt_alpha_bar * x + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "完整的训练流程：\n",
    "1、随机采样时间步 t  \n",
    "2、对图像添加噪声，获得带噪声的图像和噪声  \n",
    "3、使用模型预测噪声  \n",
    "4、计算预测噪声和真实噪声之间的MSE损失  \n",
    "5、反向传播和优化\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3aab5f",
   "metadata": {},
   "source": [
    "#### 采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8185067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "采样过程的思路为，从标准正态分布中采样初始噪声，然后逐步去噪，\n",
    "从 t=T 到 t=0，最后将最终结果裁剪到 [-1, 1] 范围。\n",
    "\n",
    "在去噪过程中，需要获取采样需要的系数，在之前的NoiseScheduler类中定义了这些系数\n",
    "\"\"\"\n",
    "\n",
    "def sample(model, scheduler, num_samples, size, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 从标准正态分布采样初始噪声 x_T ~ N(0, I)\n",
    "        x_t = torch.randn(num_samples, *size).to(device)\n",
    "\n",
    "        # 逐步去噪，从 t=T 到 t=0\n",
    "        for t in reversed(range(scheduler.num_steps)):\n",
    "            # 构造时间步batch\n",
    "            t_batch = torch.tensor([t] * num_samples).to(device)\n",
    "\n",
    "            # 获取采样需要的系数\n",
    "            sqrt_recip_alpha_bar = scheduler.get(scheduler.sqrt_recip_alphas_bar, t_batch, x_t.shape)\n",
    "            sqrt_recipm1_alpha_bar = scheduler.get(scheduler.sqrt_recipm1_alphas_bar, t_batch, x_t.shape)\n",
    "            posterior_mean_coef1 = scheduler.get(scheduler.posterior_mean_coef1, t_batch, x_t.shape)\n",
    "            posterior_mean_coef2 = scheduler.get(scheduler.posterior_mean_coef2, t_batch, x_t.shape)\n",
    "\n",
    "            # 预测噪声\n",
    "            predicted_noise = model(x_t, t_batch)\n",
    "\n",
    "            # 计算x_0的预测值：x_0 = 1/sqrt(α_bar_t) * x_t - sqrt(1/α_bar_t-1) * ε_θ(x_t,t)\n",
    "            _x_0 = sqrt_recip_alpha_bar * x_t - sqrt_recipm1_alpha_bar * predicted_noise\n",
    "            \n",
    "            # 计算后验分布均值 μ_θ(x_t,t)\n",
    "            model_mean = posterior_mean_coef1 * _x_0 + posterior_mean_coef2 * x_t\n",
    "            \n",
    "            # 计算后验分布方差的对数值 log(σ_t^2)\n",
    "            model_log_var = scheduler.get(\n",
    "                torch.log(\n",
    "                    torch.cat([scheduler.posterior_var[1:2], scheduler.betas[1:]]),\n",
    "                    t_batch, x_t.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if t > 0:\n",
    "                # t > 0时从后验分布采样：x_t-1 = μ_θ(x_t,t) + σ_t * z, z~N(0,I)\n",
    "                noise = torch.rand_like(x_t).to(device)\n",
    "                x_t = model_mean + torch.exp(0.5 * model_log_var) * noise\n",
    "            else:\n",
    "                # t = 0时直接使用均值作为生成结果\n",
    "                x_t = model_mean\n",
    "            \n",
    "        # 将最终结果裁剪到[-1, 1]的范围\n",
    "        x_0 = torch.clamp(x_t, -1.0, 1.0)\n",
    "    \n",
    "    return x_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b7f26",
   "metadata": {},
   "source": [
    "#### 评估："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用预训练好的模型，获得真实图像和生成图像的特征\n",
    "\"\"\"\n",
    "class InceptionStatistics:\n",
    "    pass\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "Inception Score（IS）\n",
    "IS 分数通过预训练的网络评估生成图像的质量和多样性。\n",
    "\n",
    "IS 分数越高说明：\n",
    "1、每张生成图像的类别预测越清晰（质量好）\n",
    "2、不同图像的类别分布越分散（多样性好）\n",
    "\n",
    "具体步骤：\n",
    "1、将所有图像分为 batch\n",
    "2、对每组计算：\n",
    "    · 计算边缘分布 p(y)，即对当前 batch 的p(y|x) 取平均\n",
    "    · 计算 KL 散度\n",
    "    · 取指数\n",
    "3、返回所有组得分的均值和标准差\n",
    "\"\"\"\n",
    "\n",
    "def calculate_inception_score(probs, splits=10):\n",
    "    # 存储每个splits的 IS 分数\n",
    "    scores = []\n",
    "    # 计算每个split的大小\n",
    "    split_size = probs[0] // splits\n",
    "\n",
    "    # 对每个split进行计算\n",
    "    for i in range(splits):\n",
    "        # 获取当前split的概率分布\n",
    "        part = probs[i * split_size: (i + 1) * split_size]\n",
    "        \n",
    "        # 计算KL散度：KL(p(y|x) || p(y))\n",
    "        kl = part * (\n",
    "            np.log(part) - \n",
    "            np.log(\n",
    "                np.expand_dims(\n",
    "                    np.mean(part, axis=0), 0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 对每个样本的KL散度求平均\n",
    "        kl = np.mean(np.sum(kl, axis=1))\n",
    "\n",
    "        # 计算 exp(KL) 并添加到scores列表\n",
    "        scores.append(np.exp(kl))\n",
    "    \n",
    "    # 返回所有split的IS分数的均值和标准差\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "Fréchet Inception Distance (FID)\n",
    "FID 分数通过比较真实图像和生成图像在网络特征空间的分布来评估生成质量。\n",
    "\n",
    "FID 分数越低说明图像的特征分布越接近真实图像分布，生成质量越好。\n",
    "\n",
    "具体步骤为：\n",
    "1、分别对真实图像和生成图像：\n",
    "    · 通过模型提取特征\n",
    "    · 计算特征和均值向量和协方差矩阵\n",
    "2、计算均值向量之间的欧氏距离\n",
    "3、计算协方差矩阵的平方根项\n",
    "4、计算最终的 FID 分数\n",
    "\"\"\"\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    # 计算真实图像和生成图像特征的均值向量和协方差矩阵\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "\n",
    "    # 计算均值向量之间的欧几里得距离的平方\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "\n",
    "    # 计算协方差矩阵的平方根项：(Σ_r Σ_f)^(1/2)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    # 如果结果包含复数,取其实部\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    # 计算最终的 FID 分数\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "    return fid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
