### 一、**参数高效微调（PEFT）**  
这类方法通过最小化调整参数的数量和计算成本，实现模型性能提升，尤其适合资源受限的场景：  
1. **LoRA（低秩适应）**  
   - 核心思想：在模型的权重矩阵中引入低秩矩阵，仅训练这些小矩阵而不更新原始参数。例如，在视觉语言模型中，LoRA可针对注意力机制模块（如`q_proj`、`v_proj`等）进行适配。  
   - 实战案例：在微调Google的PaliGemma 2模型时，通过LoRA结合4-bit量化技术，在单块A100 GPU上完成训练，显著降低显存需求。  
2. **QLoRA（量化低秩适应）**  
   - 结合LoRA与4-bit量化技术，将模型参数压缩后反量化训练，适用于超大规模模型的高效微调。  
3. **适配器调整（Adapter Tuning）**  
   - 在模型层间插入小型神经网络模块，仅训练这些模块。例如，在视觉任务中，适配器可处理跨模态特征融合。  

---

### 二、**监督微调（SFT）与混合训练**  
1. **传统监督微调**  
   - 直接使用标注数据对模型进行端到端训练，但需要大量高质量数据。例如，微调视觉问答任务时需标注的（图像-问题-答案）三元组。  
2. **迭代监督微调（Iterative SFT）**  
   - 分阶段调整模型，逐步融合视觉与文本表征。昆仑万维的R1V模型结合迭代SFT与GRPO强化学习，提升跨模态推理能力。  

---

### 三、**强化学习微调（RLHF/RFT）**  
1. **基于规则的奖励机制**  
   - 如DeepSeek的Visual-RFT方法，针对视觉任务（目标检测、细分类）设计可验证奖励（如IoU交并比、分类准确率），通过强化学习优化模型。仅需10~1000条样本即可显著提升性能。  
2. **人类反馈强化学习（RLHF）**  
   - 通过奖励模型（RM）对齐人类偏好。例如，阿里Qwen2.5-VL-32B通过强化学习优化回答的准确性和人类偏好对齐，在数学推理和视觉解析任务中表现优异。  

---

### 四、**多模态混合训练策略**  
1. **动态特征建模与状态变化网络**  
   - 清华与哈佛提出的4D LangSplat框架，利用多模态大模型生成对象级语言描述，并通过状态变化网络建模动态语义场，实现时间敏感查询任务的高效处理。  
2. **跨模态蒸馏与自适应推理链**  
   - 昆仑万维R1V通过自适应长度思维链蒸馏，动态控制推理步骤，避免“过度思考”，提升复杂视觉任务的推理效率。  

---

### 五、**领域特定优化与开源工具**  
1. **开源框架与实战工具**  
   - Visual-RFT项目提供完整的训练代码和评测基准，支持视觉定位、开放目标检测等任务。  
   - Hugging Face生态支持多模态模型（如PaliGemma、Qwen-VL）的LoRA微调，提供标准化接口。  
2. **少样本与开放场景适配**  
   - 针对数据稀缺场景，Visual-RFT和R1V均展示了少样本微调能力，例如检测动漫角色或医学影像分析。  

---

### **方法选择建议**  
- **少样本场景**：优先选择强化学习微调（如Visual-RFT）或参数高效方法（LoRA）。  
- **高精度需求**：结合监督微调与强化学习（如Qwen2.5-VL的混合优化）。  
- **动态任务处理**：参考4D LangSplat的动态语义场建模，适用于视频或时序数据。  
